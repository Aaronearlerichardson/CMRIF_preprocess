#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import argparse
import sys
import re
import shutil
import json
import subprocess
import gzip
import numpy as np
import nibabel as nib
import csv
import subprocess
from pathlib import Path
import pydicom as dicom

def get_parser(): #parses flags at onset of command
    parser = argparse.ArgumentParser(
        formatter_class=argparse.RawDescriptionHelpFormatter
        , description="""
         Data2bids is a script based on the SIMEXP lab script to convert nifti MRI files into BIDS format. This script has been modified to 
         also parse README data as well as include conversion of DICOM files to nifti. The script utilizes Chris Rorden's Dcm2niix program for 
         actual conversion. 

         This script takes one of two formats for conversion. The first is a series of DICOM files in sequence with an optional \"medata\" folder which
         contains any number of single or multi-echo uncompressed nifti files (.nii). Note that nifti files in this case must also have a corresponding 
         DICOM scan run, but not necessarily scan echo (for example, one DICOM scan for run 5 but three nifti files which are echoes 1, 2, and 3 of
         run 5). The other format is a series of nifti files and a README.txt file formatted the same way as it is in the example. Both formats are 
         shown in the examples folder.

         Both formats use a .json config file that maps either DICOM tags or text within the nifti file name to BIDS metadata. The syntax and formatting of this .json file 
         can be found here https://github.com/SIMEXP/Data2Bids#heuristic .

         The only thing this script does not account for is event files. If you have the 1D files that's taken care of, but chances are you have some other 
         format. If this is the case, I recommend https://bids-specification.readthedocs.io/en/stable/04-modality-specific-files/05-task-events.html
         so that you can make BIDS compliant event files.

         Data2bids documentation at https://github.com/SIMEXP/Data2Bids
         Dcm2niix documentation at https://github.com/rordenlab/dcm2niix"""
        , epilog="""
            Made by Aaron Earle-Richardson (ame224@cornell.edu)
            """)

    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument(
        "-i"
        , "--input_dir"
        , required=False
        , default=None
        , help="""
        Input data directory(ies), must include a readme.txt file formatted like example under examples folder. 
        Mutually exclusive with DICOM directory option. Default: current directory
        """,
        )

    parser.add_argument(
        "-c"
        , "--config"
        , required=False
        , default=None
        , help="JSON configuration file (see https://github.com/SIMEXP/Data2Bids/blob/master/example/config.json)",
        )

    parser.add_argument(
        "-o"
        , "--output_dir"
        , required=False
        , default=None
        , help="Output BIDS directory, Default: Inside current directory ",
        )

    group.add_argument(
        "-d"
        ,"--DICOM_path"
        , default=None
        , required=False
        , help="Optional DICOM directory, Mutually exclusive with input directory option",
        )

    parser.add_argument(
        "-m"
        ,"--multi_echo"
        , nargs='*'
        , type=int
        , required=False
        , help="""
        indicator of multi-echo dataset. Only necessary if NOT converting DICOMs. For example, if runs 3-6 were all multi-echo then the flag
        should look like: -m 3 4 5 6 . Additionally, the -m flag may be called by itself if you wish to let data2bids auto-detect multi echo data,
        but it will not be able to tell you if there is a mistake."""
        )

    parser.add_argument(
        "-verb"
        , "--verbose"
        , required=False
        , action='store_true' 
        , help="verbosity",
        )

    return(parser)

class Data2Bids(): #main conversion and file organization program

    def __init__(self, input_dir=None, config=None, output_dir=None, DICOM_path=None, multi_echo=None, verbose=False): #sets the .self globalization for self variables
        self._input_dir = None
        self._config_path = None
        self._config = None
        self._bids_dir = None
        self._bids_version = "1.2.0"
        self._dataset_name = None

        self.set_data_dir(input_dir,DICOM_path)
        self.set_config_path(config)
        self.set_bids_dir(output_dir)
        self.set_DICOM(DICOM_path)
        self.set_multi_echo(multi_echo)
        self.set_verbosity(verbose)

    def set_verbosity(self,verbose):
        self._is_verbose = verbose

    def set_multi_echo(self,multi_echo): #if -m flag is called
        if multi_echo is None:
            self.is_multi_echo = False
        else:
            self.is_multi_echo = True
            if not multi_echo:
                self._multi_echo = 0
            else:
                self._multi_echo = multi_echo

    def set_DICOM(self, ddir): #triggers only if dicom flag is called and therefore _data_dir is None
        if self._data_dir is None:
            self._data_dir = os.path.dirname(self._bids_dir)
            subdirs = [x[0] for x in os.walk(ddir)]
            files = [x[2] for x in os.walk(ddir)]
            sub_num = str(dicom.read_file(os.path.join(subdirs[1],files[1][0]))[0x10, 0x20].value).split("_",1)[1]
            sub_dir = os.path.join(os.path.dirname(self._bids_dir),"sub-{SUB_NUM}".format(SUB_NUM=sub_num)) #destination subdirectory
            if os.path.isdir(sub_dir):
                proc = subprocess.Popen("rm -rf {file}".format(file=sub_dir), shell=True, stdout=subprocess.PIPE)
                proc.communicate()
            os.mkdir(sub_dir)

            if any("medata" in x for x in subdirs):     #copy over and list me data
                melist = [x[2] for x in os.walk(os.path.join(ddir,"medata"))][0]
                runlist = []
                for me in melist:
                    if me.startswith("."):
                        continue
                    runmatch = re.match(r".*run(\d{2}).*",me).group(1)
                    if str(int(runmatch)) not in runlist:
                        runlist.append(str(int(runmatch)))
                    shutil.copyfile(os.path.join(ddir,"medata",me),os.path.join(sub_dir,me))
                self.is_multi_echo = True #will trigger even if single echo data is in medata folder. Should still be okay

            for subdir in subdirs[1:]: #not including parent folder or /medata, run dcm2niix on non me data
                try:
                    fobj = dicom.read_file(os.path.join(subdir, list(os.walk(subdir))[0][2][0]),force=True) #first dicom file of the scan
                    scan_num = str(int(os.path.basename(subdir))).zfill(2)
                except ValueError:
                    continue
                firstfile = [x[2] for x in os.walk(subdir)][0][0]
                #print(str(fobj[0x20, 0x11].value), runlist)
                # running dcm2niix, 
                if str(fobj[0x20, 0x11].value) in runlist:
                    proc = subprocess.Popen("dcm2niix -z y -f run{SCAN_NUM}_%p_%t_sub{SUB_NUM} -o {OUTPUT_DIR} -s y -b y {DATA_DIR}".format(
                        OUTPUT_DIR=sub_dir, SUB_NUM=sub_num, DATA_DIR=os.path.join(subdir,firstfile), SCAN_NUM=scan_num), shell=True, stdout=subprocess.PIPE)
                    #output = proc.stdout.read()
                    outs, errs = proc.communicate()
                    prefix = re.match(".*/sub-{SUB_NUM}/(run{SCAN_NUM}".format(SUB_NUM=sub_num,SCAN_NUM=scan_num) + "[^ \(\"\\n\.]*).*",
                        str(outs)).group(1)
                    for file in os.listdir(sub_dir):
                        mefile = re.match("run{SCAN_NUM}(\.e\d\d)\.nii".format(SCAN_NUM=scan_num),file)
                        if re.match("run{SCAN_NUM}\.e\d\d.nii".format(SCAN_NUM=scan_num),file):
                            shutil.move(os.path.join(sub_dir,file),os.path.join(sub_dir,prefix + mefile.group(1) + ".nii"))
                            shutil.copy(os.path.join(sub_dir,prefix + ".json"),os.path.join(sub_dir,prefix + mefile.group(1) + ".json"))
                    os.remove(os.path.join(sub_dir,prefix + ".nii.gz"))
                    os.remove(os.path.join(sub_dir,prefix + ".json"))
                else:
                    proc = subprocess.Popen("dcm2niix -z y -f run{SCAN_NUM}_%p_%t_sub{SUB_NUM} -o {OUTPUT_DIR} -b y {DATA_DIR}".format(
                        OUTPUT_DIR=sub_dir, SUB_NUM=sub_num, DATA_DIR=subdir, SCAN_NUM=scan_num), shell=True, stdout=subprocess.PIPE)
                    outs, errs = proc.communicate()
                sys.stdout.write(outs.decode("utf-8"))

            self._multi_echo = runlist
            self._data_dir = os.path.join(os.path.dirname(self._bids_dir), "sub-{SUB_NUM}".format(SUB_NUM=sub_num))
        self._DICOM_path = ddir

    def get_data_dir(self):
        return self._data_dir

    def set_data_dir(self, data_dir,DICOM): #check if input dir is listed
        if DICOM is None:
            if data_dir is None:
                self._data_dir = os.getcwd()
            else:
                self._data_dir = data_dir
            self._dataset_name = os.path.basename(self._data_dir)
        else:
            self._data_dir = None

    def get_config(self):
        return self._config

    def get_config_path(self):
        return self._config_path

    def _set_config(self):
        with open(self._config_path, 'r') as fst:
            self._config = json.load(fst)

    def set_config(self, config):
        self._config = config

    def set_config_path(self, config_path):
        if config_path is None:
            # Checking if a config.json is present
            if os.path.isfile(os.path.join(os.getcwd(), "config.json")):
                self._config_path = os.path.join(os.getcwd(), "config.json")
            # Otherwise taking the default config
            else:
                self._config_path = os.path.join(os.path.dirname(__file__), "config.json")
        else:
            self._config_path = config_path

        self._set_config()

    def get_bids_dir(self):
        return self._bids_dir

    def set_bids_dir(self, bids_dir):
        if bids_dir is None:
            # Creating a new directory for BIDS
            try:
                self._bids_dir = os.path.join(self._data_dir, self._dataset_name + "_BIDS")
            except TypeError:
                print("Error: Please provide input data directory if no BIDS directory...")
        else: #deleting old BIDS to make room for new 
            if not os.path.basename(os.path.normpath(bids_dir)) == "BIDS":
                newdir = os.path.join(bids_dir,"BIDS")
            else:
                newdir = bids_dir
            if os.path.isdir(newdir):
                proc = subprocess.Popen("rm -rf {file}".format(file=newdir), shell=True, stdout=subprocess.PIPE)
                proc.communicate()
            os.mkdir(newdir)
            bids_dir = newdir
        self._bids_dir = bids_dir

    def get_bids_version(self):
        return self._bids_version

    def match_regexp(self, config_regexp, filename, subtype=False):
        delimiter_left = config_regexp["left"]
        delimiter_right = config_regexp["right"]
        match_found = False

        if subtype:
            for to_match in config_regexp["content"]:
                if re.match(".*?"
                            + delimiter_left
                            + '(' + to_match[1] + ')'
                            + delimiter_right
                            + ".*?", filename):
                    match = to_match[0]
                    match_found = True
        else:
            for to_match in config_regexp["content"]:
                if re.match(".*?"
                            + delimiter_left
                            + '(' + to_match + ')'
                            + delimiter_right
                            + ".*?", filename):
                    match = re.match(".*?"
                                     + delimiter_left
                                     + '(' + to_match + ')'
                                     + delimiter_right
                                     + ".*?", filename).group(1)
                    match_found = True
        assert match_found
        return match

    def bids_validator(self):
        assert self._bids_dir is not None, "Cannot launch bids-validator without specifying bids directory !"
        #try:
        subprocess.check_call(['bids-validator', self._bids_dir])
        #except FileNotFoundError:
        #    print("bids-validator does not appear to be installed")

    def generate_names(self, part_match, src_file_path, filename, namelist=None): #function to run through name text and generate metadata
        sess_match = None
        ce_match = None
        acq_match = None
        echo_match = None
        data_type_match = None
        task_label_match = None
        run_match = None
        dst_file_path = self._bids_dir + "/sub-" + part_match
        new_name = "/sub-" + part_match
        SeqType = None
        # Matching the session
        try: 
            SeqType = str(self.match_regexp(self._config["pulseSequenceType"], filename, subtype=True))
        except AssertionError:
            print("No pulse sequence found for %s" %src_file_path)
        try:
            sess_match = self.match_regexp(self._config["sessLabel"], filename)
            dst_file_path = dst_file_path + "/ses-" + sess_match
            new_name = new_name + "_ses-" + sess_match
        except AssertionError:
            print("No session found for %s" %src_file_path)
            return

        # Matching the anat/fmri data type and task
        try:
            data_type_match = self.match_regexp(self._config["anat"]
                                                ,filename
                                                , subtype=True)
            dst_file_path = dst_file_path + "/anat"

            try:
                acq_match = self.match_regexp(self._config["anat.acq"]
                                              ,filename
                                              , subtype=True)
                new_name = new_name + "_acq-" + acq_match
            except AssertionError:
                print("no optional labels for %s" %src_file_path)
                try: 
                    ce_match = self.match_regexp(self._config["anat.ce"]
                                                 ,filename
                                                 , subtype=True)
                    new_name = new_name + "_ce-" + ce_match
                except AssertionError:
                    print("no special contrast labels for %s" %src_file_path)
        except AssertionError:
            # If no anatomical, trying functionnal
            try:
                data_type_match = self.match_regexp(self._config["func"]
                                                    ,filename
                                                    , subtype=True)
                dst_file_path = dst_file_path + "/func"
                # Now trying to match the task
                try:
                    task_label_match = self.match_regexp(self._config["func.task"]
                                                         ,filename
                                                         , subtype=True)
                    new_name = new_name + "_task-" + task_label_match
                except AssertionError:
                    print("No task found for %s" %src_file_path)
                    return
            except AssertionError:
                print("No anat nor func data type found for %s" %src_file_path)
                return


        # Matching the run number
        try:
            run_match = self.match_regexp(self._config["runIndex"],filename)
            new_name = new_name + "_run-" + run_match
        except AssertionError:
            print("No run found for %s" %src_file_path)

        if self.multi_echo_check(run_match,filename):
            try:
                echo_match = self.match_regexp(self._config["echo"],filename)
                new_name = new_name + "_echo-" + echo_match 
            except AssertionError:
                print("No echo found for %s" %src_file_path)
                return
         
	# Adding the modality to the new filename
        new_name = new_name + "_" + data_type_match

        return(new_name,dst_file_path,run_match,
               acq_match,echo_match,sess_match,ce_match,
               data_type_match, task_label_match, SeqType)
        
    def slice_time_calc(self, TR, sNum, totNum, delay):
        intervaltime = (TR-delay) / totNum
        tslice = delay + ((sNum) * intervaltime)
        return(tslice)
    
    def multi_echo_check(self,runnum,src_file=""): # check to see if run is multi echo based on input
        if self.is_multi_echo:
            if int(runnum) in self._multi_echo: 
                return(True)
            else:
                if self._multi_echo == 0:
                    try:
                        self.match_regexp(self._config["echo"],src_file)
                    except AssertionError:
                        return(False)
                    return(True)
                else:
                    return(False)
        else:
            return(False)

    def get_params(self,folder,echo_num,run_num): #function to run through DICOMs and get metadata
        #threading?
        if self.is_multi_echo and run_num in self._multi_echo:
            vols_per_time = len(self._config['delayTimeInSec'])-1
            echo = self._config['delayTimeInSec'][echo_num]
        else:
            vols_per_time = 1
            echo = None
            
        for root, _, dfiles in os.walk(folder, topdown=True):
            dfiles.sort()
            for dfile in dfiles:
                dcm_file_path = os.path.join(root, dfile)
                fobj = dicom.read_file(str(dcm_file_path))
                if echo is None:
                    try:
                        echo = float(fobj[0x18, 0x81].value) / 1000
                    except KeyError:
                        echo = self._config['delayTimeInSec'][0]
                ImagesInAcquisition = int(fobj[0x20, 0x1002].value)
                seqlist=[]
                for i in list(range(5)):
                    try:
                        seqlist.append(fobj[0x18, (32+i)].value)
                        if seqlist[i] == 'NONE':
                            seqlist[i] = None
                        if isinstance(seqlist[i], dicom.multival.MultiValue):
                            seqlist[i] = list(seqlist[i])
                        if isinstance(seqlist[i], list):
                            seqlist[i] = ", ".join(seqlist[i])
                    except KeyError:
                        seqlist.append(None)
                [ScanningSequence, SequenceVariant, SequenceOptions, AquisitionType, SequenceName] = seqlist
                try:
                    timings
                except NameError:
                    timings = [None] * int(ImagesInAcquisition/vols_per_time)
                '''
                if [0x20, 0x9057] in fobj:
                InStackPositionNumber = int(fobj[0x20, 0x9057].value)
                else:
                InStackPositionNumber = None
                AquisitionDuration = ((float(fobj[0x19, 0x105a].value)/1000000)) #extracted in microseconds, converted to seconds
                InstanceNumber = int(fobj[0x20, 0x13].value)
                SliceLocation = float(fobj[0x20, 0x1041].value)
                '''
                RepetitionTime = ((float(fobj[0x18, 0x80].value)/1000)) #TR value extracted in milliseconds, converted to seconds
                try:
                    acquisition_series = self._config['series']
                except KeyError: 
                    print("default")
                    acquisition_series = "non-interleaved"
                if acquisition_series == "even-interleaved":
                    InstackPositionNumber = 2
                else:
                    InStackPositionNumber = 1
                InstanceNumber = 0
                while None in timings:
                    if timings[InStackPositionNumber-1] is None:
                        timings[InStackPositionNumber-1] = self.slice_time_calc(RepetitionTime, InstanceNumber,
                                                                            int(ImagesInAcquisition/vols_per_time),echo)
                    if acquisition_series == "odd-interleaved" or acquisition_series == "even-interleaved" :
                        InStackPositionNumber +=2
                        if InStackPositionNumber > ImagesInAcquisition/vols_per_time and acquisition_series == "odd-interleaved":
                            InStackPositionNumber = 2
                        elif InStackPositionNumber > ImagesInAcquisition/vols_per_time and acquisition_series == "even-interleaved":
                            InStackPositionNumber = 1
                    else:
                        InStackPositionNumber += 1
                    InstanceNumber += 1
                return(timings,echo,ScanningSequence, SequenceVariant, SequenceOptions, SequenceName) 

    def set_default(self, obj):
        if isinstance(obj, set):
            return list(obj)
        raise TypeError

    def force_remove(self, func, mypath, excinfo):

        if os.path.isfile(mypath):
            os.remove(mypath)
        try:
            if os.path.isdir(mypath):
                self.delete_folder(Path(mypath))
        except OSError:
            shutil.rmtree(mypath,ignore_errors=True)
        func(mypath)

    def delete_folder(self, pth):
        for sub in pth.iterdir() :
            if sub.is_dir() :
                self.delete_folder(sub)
            else :
                sub.unlink()
        pth.rmdir()

    def run(self): #main function

        # First we check that every parameters are configured
        if (self._data_dir is not None
            and self._config_path is not None
            and self._config is not None
            and self._bids_dir is not None):

            print("---- data2bids starting ----")
            print(self._data_dir)
            print("\n BIDS version:")
            print(self._bids_version)
            print("\n Config from file :")
            print(self._config_path)
            print("\n Ouptut BIDS directory:")
            print(self._bids_dir)
            print("\n")

            # Create the output BIDS directory
            if not os.path.exists(self._bids_dir):
                os.makedirs(self._bids_dir)
            #else
            #    shutil.rmtree(self._bids_dir)
            

            # What is the base format to convert to
            curr_ext = self._config["dataFormat"]
            compress = self._config["compress"]

            # delay time in TR unit (if delay_time = 1, delay_time = repetition_time)
            repetition_time = self._config["repetitionTimeInSec"]
            delaytime = self._config["delayTimeInSec"]

            #dataset_description.json must be included in the folder root foolowing BIDS specs
            if os.path.exists(self._bids_dir + "/dataset_description.json"):
                #with open(dst_file_path + new_name + ".json", 'w') as fst:
                with open(self._bids_dir + "/dataset_description.json") as fst:
                    filedata=json.load(fst)
                with open(self._bids_dir + "/dataset_description.json", 'w') as fst:
                    data = {'Name': self._dataset_name,
                               'BIDSVersion': self._bids_version}
                    filedata.update(data)
                    json.dump(filedata, fst, ensure_ascii=False, indent=4)
            else:
                with open(self._bids_dir + "/dataset_description.json", 'w') as fst:
                    data = {'Name': self._dataset_name,
                               'BIDSVersion': self._bids_version}
                    json.dump(data, fst, ensure_ascii=False, indent = 4)
            # now we can scan all files and rearrange them
            names_list = []
            tsv_condition_runs = []
            tsv_fso_runs = []
            dst_file_path_list = []
            d_list = []
            part_match = None
            run_list = []
            for root, _, files in os.walk(self._data_dir, topdown=True):
                files.sort()
                while files:
                    file=files.pop(0)
                    #print(file)
                    src_file_path = os.path.join(root, file)
                    dst_file_path = self._bids_dir
                    data_type_match = None
                    new_name = None

                    #if re.match(".*?" + "imaginary" + ".*?" ,file):
                    #    continue
                    if re.match(".*?" + ".json", file):
                        try:
                            part_match = self.match_regexp(self._config["partLabel"], file)
                        except AssertionError:
                            raise SyntaxError("file: {filename} has no matching {config}".format(filename=file,config=self._config["content"][:][0]))
                        try:
                            (new_name,dst_file_path, run_match, _,echo_match,sess_match,_,
                             data_type_match,task_label_match,SeqType) = self.generate_names(part_match, src_file_path, file)
                        except TypeError:
                            continue
                        if echo_match is None:
                            echo_match=0
                        if new_name in names_list:
                            shutil.copy(src_file_path, dst_file_path + new_name + ".json")
                            # finally, if it is a bold experiment, we need to edit the JSON file using DICOM tags
                            if os.path.exists(dst_file_path + new_name + ".json"):
                                ### https://github.com/nipy/nibabel/issues/712, that is why we take the
                                ### scanner parameters from the config.json
                                #nib_img = nib.load(src_file_path)
                                #TR = nib_img.header.get_zooms()[3]
                                for foldername in os.listdir(str(self._DICOM_path)):
                                    if run_match.zfill(4) == foldername.zfill(4):
                                        DICOM_filepath = os.path.join(self._DICOM_path, foldername)
                                        slicetimes, echotime, ScanSeq, SeqVar, SeqOpt, SeqName = self.get_params(str(DICOM_filepath), int(echo_match), int(run_match))
                                    
                                    #with open(dst_file_path + new_name + ".json", 'w') as fst:
                                with open(dst_file_path + new_name + ".json", 'r+') as fst:
                                    filedata=json.load(fst)
                                with open(dst_file_path + new_name + ".json", 'w') as fst:
                                    if data_type_match == "bold":
                                        filedata['TaskName'] = task_label_match
                                        filedata['SliceTiming'] = slicetimes
                                        if int(run_match) in self._multi_echo :
                                            filedata['EchoTime'] = echotime
                                        else:
                                            filedata['DelayTime'] = delaytime[0]
                                    if SeqType is not None:
                                        filedata['PulseSequenceType'] = SeqType
                                    if ScanSeq is not None:
                                        filedata['ScanningSequence'] = ScanSeq
                                    if SeqVar is not None: 
                                        filedata['SequenceVariant'] = SeqVar
                                    if SeqOpt is not None:
                                        filedata['SequenceOption'] = SeqOpt
                                    if SeqName is not None:
                                        filedata['SequenceName'] = SeqName
                                    json.dump(filedata, fst, ensure_ascii=False, indent=4, default=self.set_default)
                            else:
                                print("Cannot update %s" %(dst_file_path + new_name + ".json"))
                        elif any(re.search(".nii",filelist) for filelist in files):
                            files.append(src_file_path)
                            part_match = ""
                        continue
                    elif re.match(".*?" + "EADME.txt", file):  #if README.txt in image list
                        with open(src_file_path, 'r') as readmetext:
                            for line in readmetext:
                                regret_words = ["Abort","NOTE"]
                                if ". tempAttnAudT" in line:  #these lines could and should be improved by linking config["func.task"] instead of literal strings
                                    prevline="con"
                                    tsv_condition_runs.append(re.search(r'\d+', line).group()) #save the first number on the line
                                elif ". fsoSubLocal" in line:
                                    prevline="fso"
                                    tsv_fso_runs.append(re.search(r'\d+', line).group())
                                elif all(x in line for x in regret_words):
                                    if prevline is "con":
                                        del tsv_condition_runs[-1]
                                    elif prevline is "fso":
                                        del tsv_fso_runs[-1]
                                    prevline=""
                                else:
                                    prevline=""
                        if part_match is not None:   
                            if not os.path.exists(self._bids_dir + "/sub-" + part_match): #Writing both a particpant-specific and agnostic README. Requires creation of a .bidsignore file for local READMEs
                                os.makedirs(self._bids_dir + "/sub-" + part_match)
                            shutil.copy(src_file_path, self._bids_dir + "/sub-" + part_match + "/README.txt")
                            with open(src_file_path, 'r') as readmetext:
                                for line in readmetext:
                                    if os.path.exists(self._bids_dir + "/README"):
                                        with open(self._bids_dir + "/README", 'a') as f:
                                            f.write(line + "\n")
                                    else:
                                        with open(self._bids_dir + "/README", 'w') as f:
                                            f.write(line + "\n")
                            if not os.path.exists(self._bids_dir + "/.bidsignore"):
                                with open(self._bids_dir + "/.bidsignore", 'w') as f:
                                    f.write("*.txt")
                        else: 
                            files.append(src_file_path)
                        continue
                    elif re.match(".*?" + ".1D", file): 
                        d_list.append(src_file_path)
                        continue
                    # if the file doesn't match the extension, we skip it
                    elif not any(re.match(".*?" + ext, file) for ext in curr_ext):
                        print("Warning : Skipping %s" %src_file_path)
                        continue
                    if self._is_verbose:
                        print("trying %s" %src_file_path)
                    
                    # Matching the participant label to determine if there exists therein delete previously created BIDS subject files
                    try:
                        part_match = self.match_regexp(self._config["partLabel"], file)
                        if os.path.exists(self._bids_dir + "/sub-" + part_match) and not any("sub-" + part_match in x for x in names_list):
                            print("Deleting old BIDS directory for subject %s" %part_match)
                            shutil.rmtree(self._bids_dir + "/sub-" + part_match,onerror=self.force_remove)
                    except AssertionError:
                        print("No participant found for %s" %src_file_path)
                        continue
                    except OSError: #problem spot, may miss deleting some files causing them to erroneously carry over
                        shutil.rmtree(self._bids_dir + "/sub-" + part_match,ignore_errors=True) 
                
                    try:
                        (new_name,dst_file_path,_,
                         _,echo_match,sess_match,_,
                         data_type_match,task_label_match,_) = self.generate_names(part_match, src_file_path, file)
                    except TypeError as problem:
                        print("huh?")
                        print("problem with %s:" %src_file_path, problem)
                        continue

                    # Creating the directory where to store the new file
                    if not os.path.exists(dst_file_path):
                        os.makedirs(dst_file_path)

                    # finally, if the file is not nifti, we convert it using nibabel
                    if not ".nii" in curr_ext or not ".nii.gz" in curr_ext:
                        # loading the original image
                        nib_img = nib.load(src_file_path)
                        nib_affine = np.array(nib_img.affine)
                        nib_data = np.array(nib_img.dataobj)

                        # create the nifti1 image
                        # if minc format, invert the data and change the affine transformation
                        # there is also an issue on minc headers
                        if curr_ext == ".mnc":
                            if len(nib_img.shape) > 3:
                                nib_affine[0:3, 0:3] = nib_affine[0:3, 0:3] 
                                rot_z(np.pi/2) 
                                rot_y(np.pi) 
                                rot_x(np.pi/2)
                                nib_data = nib_data.T
                                nib_data = np.swapaxes(nib_data, 0, 1)

                                nifti_img = nib.Nifti1Image(nib_data, nib_affine, nib_img.header)
                                nifti_img.header.set_xyzt_units(xyz="mm", t="sec")
                                zooms = np.array(nifti_img.header.get_zooms())
                                zooms[3] = repetition_time
                                nifti_img.header.set_zooms(zooms)
                            elif len(nib_img.shape) == 3:
                                nifti_img = nib.Nifti1Image(nib_data, nib_affine, nib_img.header)
                                nifti_img.header.set_xyzt_units(xyz="mm")
                        else:
                            nifti_img = nib.Nifti1Image(nib_data, nib_affine, nib_img.header)

                        #saving the image
                        nib.save(nifti_img, dst_file_path + new_name + ".nii.gz")

                    # if it is already a nifti file, no need to convert it so we just copy rename
                    elif ".nii.gz" in file:
                        shutil.copy(src_file_path, dst_file_path + new_name + ".nii.gz")
                    else:
                        shutil.copy(src_file_path, dst_file_path + new_name + ".nii")
                        #compression just if .nii files
                        if compress is True:
                            print("zipping " + file)
                            with open(dst_file_path + new_name + ".nii", 'rb') as f_in:
                                with gzip.open(dst_file_path + new_name + ".nii.gz", 'wb',9) as f_out:
                                 shutil.copyfileobj(f_in, f_out)
                            os.remove(dst_file_path + new_name + ".nii")

                    # move the sidecar from input to output

                    names_list.append(new_name)
                    dst_file_path_list.append(dst_file_path)
                    run_list.append(int(run_match))

                    

            #This section is for converting .1D files to tsv event files. If you have the 1D files that's great, but chances are you have some other 
            #format if this is the case, I recommend https://bids-specification.readthedocs.io/en/stable/04-modality-specific-files/05-task-events.html
            #so that you can make BIDS compliant event files 
            fields = list( list() for _ in range(max(run_list)))
            categories = list( list() for _ in range(max(run_list)))
            writenames = list( list() for _ in range(max(run_list)))
            n=0                                # creating variables to save and write from
            for d_file in d_list :
                #print(d_file)
                n += 1
                try:
                    category = re.search("-[0-9]{4}-[0-9]{1,2}-(.*)\\.1D", d_file).group(1) #search filename for category label
                except AttributeError:
                    try:
                        category = re.search("-[0-9]{4}-(.{1,2}-?.*)\\.1D", d_file).group(1)
                    except AttributeError:
                        print (d_file +" has no pattern matching: -####-(????)")
                        continue
                nfso = 0
                ncon = 0 
                with open(d_file) as lines: #loop through .tsv file line by line (each is a different run)
                    for line in lines:
                        if "fso-" in d_file :
                            try:
                                runnum=int(tsv_fso_runs[nfso])
                                nfso += 1 
                            except IndexError:
                                print('Error, index was %s while list was:' %str(nfso))
                                print(tsv_fso_runs)
                                continue
                        elif "condition-" in d_file:
                            try:
                                runnum=int(tsv_condition_runs[ncon])
                                ncon += 1
                            except IndexError:
                                print('Error, index was %s while list was:' %str(ncon))
                                print(tsv_condition_runs)
                                continue
                        else:
                            continue
                        i=0
                        for name in names_list:
                            try:
                                if runnum is int(re.search("run-([0-9]{2})",name).group(1)):
                                    if re.match(".*?" + "_bold" + ".*?", name):
                                        name = name.replace("_bold","")
                                        if re.match(".*?" + "_echo-[0-9]" + ".*?", name):
                                            name = name.replace(re.search("(_echo-[0-9]{2})",name).group(1),"")
                                    for field in line.strip("\\n").split(): # saving the data to variables to write later
                                        if field != '0' and field != '1':
                                            fields[runnum-1].append(float(field))
                                            categories[runnum-1].append(category)
                                            writenames[runnum-1].append(dst_file_path_list[i] + name)
                                        '''
                                        else:
                                            tempTRfields[runnum-1].append(field)
                                            tempTRcategories[runnum-1].append(category)
                                        '''
                                    break
                                i+=1
                            except AttributeError:
                                print(str(runnum) +"is not in this list:" )
            for j in range(max(run_list)): #actually writing the file
                '''
                k = 0
                for i in range(len(fields[j])): 
                    if k < len(tempTRcategories[j]):
                        if categories[j][i] == tempTRcategories[j][k]:
                            TRcategories[j].append(categories[j][i])
                            TRfields[j].append(tempTRfields[j][k])
                            k += 1
                        else:
                            TRcategories[j].append(None)
                            TRfields[j].append('0')
                        #print(len(TRfields[j]))
                '''
                if fields[j]:
                    #print(len(fields[j]), len(TRfields[j]))
                    categories[j] = [categories[j] for _,categories[j] in sorted(zip(fields[j],categories[j]))]
                    #TRfields[j] = [TRfields[j] for _,TRfields[j] in sorted(zip(fields[j],TRfields[j]))]
                    fields[j].sort()
                    #print(len(writenames[j]), len(categories[j]), len(fields[j]), len(TRfields[j]), len(tempTRcategories[j]))
                    for i in range(len(writenames[j])+1):
                        tsvnames = []
                        if self.multi_echo_check(j+1):
                            for k in range((len(delaytime)-1)):
                                tsvnames.append("_echo-" + str(k+1).zfill(2) + "_events.tsv")
                        else:
                            tsvnames.append("_events.tsv")
                        for ending in tsvnames:
                            if i == 0:
                                with open(writenames[j][0] + ending, 'a') as out_file:
                                    tsv_writer = csv.writer(out_file, delimiter='\t')           
                                    tsv_writer.writerow(['onset', 'duration', 'trial_type'])#,'TR_condition'])
                            else:
                                if i < len(writenames[j]):
                                    duration = float(fields[j][i]) - float(fields[j][i-1])
                                with open(writenames[j][i-1] + ending, 'a') as out_file:
                                    tsv_writer = csv.writer(out_file, delimiter='\t')           
                                    tsv_writer.writerow([fields[j][i-1], duration, categories[j][i-1]])#,TRfields[j][i-1]])
                    
            # Output
            if self._is_verbose:
                tree(self._bids_dir)

            # Finally, we check with bids_validator if everything went alright (This wont work)
            #self.bids_validator()

        else:
            print("Warning: No parameters are defined !")

class DisplayablePath(): # this code simply creates a tree visual to explain the BIDS file organization
    display_filename_prefix_middle = '├──'
    display_filename_prefix_last = '└──'
    display_parent_prefix_middle = '    '
    display_parent_prefix_last = '│   '

    def __init__(self, path, parent_path, is_last):
        self.path = Path(str(path))
        self.parent = parent_path
        self.is_last = is_last
        if self.parent:
            self.depth = self.parent.depth + 1
        else:
            self.depth = 0

    @property
    def displayname(self):
        if self.path.is_dir():
            return self.path.name + '/'
        return self.path.name

    @classmethod
    def make_tree(cls, root, parent=None, is_last=False, criteria=None):
        root = Path(str(root))
        criteria = criteria or cls._default_criteria

        displayable_root = cls(root, parent, is_last)
        yield displayable_root

        children = sorted(list(path
                               for path in root.iterdir()
                               if criteria(path)),
                          key=lambda s: str(s).lower())
        count = 1
        for path in children:
            is_last = count == len(children)
            if path.is_dir():
		        #yield from
    	        for i in cls.make_tree(path,
                                       parent=displayable_root,
                                       is_last=is_last,
                                       criteria=criteria):
                    yield i
            else:
                yield cls(path, displayable_root, is_last)
            count += 1

    @classmethod
    def _default_criteria(cls,path):
        return True

    def displayable(self):
        if self.parent is None:
            return self.path

        _filename_prefix = (self.display_filename_prefix_last
                            if self.is_last
                            else self.display_filename_prefix_middle)

        parts = ['{!s} {!s}'.format(_filename_prefix,
                                    self.displayname)]

        parent = self.parent
        while parent and parent.parent is not None:
            parts.append(self.display_parent_prefix_middle
                         if parent.is_last
                         else self.display_parent_prefix_last)
            parent = parent.parent

        return ''.join(reversed(parts))
#this part of the code creates the tree graphic
def tree(path):
    paths = DisplayablePath.make_tree(Path(path))
    for path_to_display in paths:
        print(path_to_display.displayable())

def rot_x(alpha):
    return np.array([[1, 0, 0]
                     , [0, np.cos(alpha), np.sin(alpha)]
                     , [0, -np.sin(alpha), np.cos(alpha)]])

def rot_y(alpha):
    return np.array([[np.cos(alpha), 0, -np.sin(alpha)]
                     , [0, 1, 0]
                     , [np.sin(alpha), 0, np.cos(alpha)]])

def rot_z(alpha):
    return np.array([[np.cos(alpha), np.sin(alpha), 0]
                     , [-np.sin(alpha), np.cos(alpha), 0]
                     , [0, 0, 1]])

def main():
    args = get_parser().parse_args()
    data2bids = Data2Bids(**vars(args))
    data2bids.run()
    
if __name__ == '__main__':
    main()
